{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68efbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis project classifies a data set of 50,000 imdb movie reviews as positive or negative based on their text review. \\nIt uses Naive Bayes, Logistic Regression, and Neural Networks.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\n",
    "\n",
    "'''\n",
    "This project classifies a data set of 50,000 imdb movie reviews as positive or negative based on their text review. \n",
    "It uses Naive Bayes, Logistic Regression, and Neural Networks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1bc2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Read Dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv', header=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b87cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.88      0.87      5016\n",
      "    positive       0.87      0.85      0.86      4984\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "\n",
    "#Preprocessing\n",
    "stopwords = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "#Isolate X and y\n",
    "X = df.review\n",
    "y = df.sentiment\n",
    "\n",
    "#Test-Train Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1337)\n",
    "\n",
    "#Apply tfidf vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)  # fit and transform the train data\n",
    "X_test = vectorizer.transform(X_test)        # transform only the test data\n",
    "\n",
    "#Train Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "predNB = naive_bayes.predict(X_test)\n",
    "\n",
    "#Print results\n",
    "print(classification_report(y_test, predNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b78dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.88      0.90      5016\n",
      "    positive       0.89      0.91      0.90      4984\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "#Train Logistic Regression\n",
    "clf = LogisticRegression(C=2.5, n_jobs=4, solver='lbfgs', random_state=17, verbose=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "predLR = clf.predict(X_test)\n",
    "\n",
    "#Print Results\n",
    "print(classification_report(y_test, predLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a36732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.87      5016\n",
      "    positive       0.87      0.88      0.88      4984\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Neural Networks\n",
    "\n",
    "#Train on data\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 10), random_state=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predNN = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff18499d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn general the most accurate approach was Logistic Regression. It had the overall best precision, recall, and f1 score.\\nThe neural network and Naive Bayes approach are both really similar, however originally when running the neural network with (15, 2)\\nlayers it resulted in a horrible ~.50 precision score, it only started matching Naive bayes at a (20, 10) layer of nodes.\\nThe Neural Network is also magnitudes slower to run since it needs more layers to get a decent accuracy.\\n\\nOverall I'd say for the current data Logistic Regression did the best, followed by Naive Bayes, then Neural Networks.\\nHowever, by changing the layers on the neural network it's possible to get even higher levels of accuracy.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write up - Analysis of various approaches\n",
    "'''\n",
    "In general the most accurate approach was Logistic Regression. It had the overall best precision, recall, and f1 score.\n",
    "The neural network and Naive Bayes approach are both really similar, however originally when running the neural network with (15, 2)\n",
    "layers it resulted in a horrible ~.50 precision score, it only started matching Naive bayes at a (20, 10) layer of nodes.\n",
    "The Neural Network is also magnitudes slower to run since it needs more layers to get a decent accuracy.\n",
    "\n",
    "Overall I'd say for the current data Logistic Regression did the best, followed by Naive Bayes, then Neural Networks.\n",
    "However, by changing the layers on the neural network it's possible to get even higher levels of accuracy.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
